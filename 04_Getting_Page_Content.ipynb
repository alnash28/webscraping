{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Page Content \n",
    "\n",
    "Now that we have determined that we need to scrape the web, how the pages are generated, and what tools we have available, the next step is to get the information from the web pages to our machine. First, we need to import the modules we need. The `requests` module should be all that you need, but the `urllib` modules are included for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import urllib.robotparser\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can I Scrape This Site?\n",
    "\n",
    "The first thing you should do is check to see if you're allowed to scrape a website. First we'll try to see if we can scrape some pages from http://quotes.toscrape.com/. Requests does not have a module for checking `robots.txt`, so we'll have to use `urllib.robotparser`. There was an effort to make a plugin called [`requests-robotstxt`](https://github.com/ambv/requests-robotstxt), but it has been inactive since 2017 and was just a proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfp = urllib.robotparser.RobotFileParser()\n",
    "rfp.set_url('http://quotes.toscrape.com/robots.txt')\n",
    "rfp.read()\n",
    "rfp.can_fetch(\"*\", \"http://quotes.toscrape.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have permission! We could check this manually as well, but 1) we want our bots to be well behaived just like their creators, and 2) it appears that they don't even have a `robots.txt` file! Looking at the code below, we see that we get a `404 Error`. Looking at the [list of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes), we see that this means \"Not Found.\"\n",
    "\n",
    "If this is the case, then you might want to do a bit more searching to see if there's any issue. Otherwise, [this article](https://serverfault.com/questions/154820/what-happens-if-a-website-does-not-have-a-robots-txt-file) seems to indicate that no `robots.txt` file means we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/robots.txt')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at another site that does have a `robots.txt` file. It's understandable that Wikipedia get's a lot of traffic. Let's see what bots are allowed to scrape the main page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IsraBot', 'Orthogaffe', '*']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "rfp = urllib.robotparser.RobotFileParser('https://en.wikipedia.org/robots.txt')\n",
    "rfp.read()\n",
    "\n",
    "bots = [\n",
    "    'MJ12bot', 'Mediapartners-Google*', 'IsraBot', 'Orthogaffe', 'UbiCrawler', 'DOC', \n",
    "    'Zao', 'sitecheck.internetseer.com', 'Zealbot', 'MSIECrawler', 'SiteSnagger',\n",
    "    'WebStripper', 'WebCopier', 'Fetch', 'Offline Explorer', 'Teleport', 'TeleportPro', \n",
    "    'WebZIP', 'linko', 'HTTrack', 'Microsoft.URL.Control', 'Xenu', 'larbin', 'libwww', \n",
    "    'ZyBORG', 'Download Ninja', 'fast', 'wget', 'grub-client', 'k2spider', 'NPBot', \n",
    "    'WebReaper', '*'\n",
    "]\n",
    "allowed = [\n",
    "    bot for bot in bots \n",
    "    if rfp.can_fetch(bot, 'https://en.wikipedia.org/')\n",
    "]\n",
    "print(allowed)\n",
    "print(rfp.can_fetch('*', 'https://en.wikipedia.org/w/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only `IsraBot`, `Orthogaffe`, and `*` are allowed to scrape. Fortunately for us, our bot is most likely going to follow the rules for `*`. The other two bots are bots built by Wikipedia. Also worth nothing, we're allowed to scrape `/`, but not `/w/`. If we try to look at that page, we'll see that `/w/` just redirects us to `/` anyway, so we're not missing much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a page\n",
    "\n",
    "Getting a page with `requests` is pretty straight forward. In fact, you already saw a quick example when we saw the `404 Error` earlier. The simplest way to **get** a web page is to use the `requests.get()` function. In the simplest form, all you have to do is pass the function the URL you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There from here, there are several different methods available to extract the response data. The best method depends on data structure.\n",
    "\n",
    "* [`response.text`](https://requests.readthedocs.io/en/master/user/quickstart/#response-content): Decoded text\n",
    "* [`response.content`](https://requests.readthedocs.io/en/master/user/quickstart/#binary-response-content): Binary response content\n",
    "* [`response.json()`](https://requests.readthedocs.io/en/master/user/quickstart/#json-response-content): JSON respone content. Saves you from having to run `json.loads()`\n",
    "* [`response.raw.read()`](https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content): Raw response content\n",
    "\n",
    "Below you can see the difference between `.text` and `.content`. BeautifulSoup will be able to accept either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Quotes to Scrape</title>\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "print(response.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<title>Quotes to Scrape</title>\\n   '\n"
     ]
    }
   ],
   "source": [
    "print(response.content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Header and Meta Data\n",
    "\n",
    "There's a pretty good chance that you're not interested in much of the header data, but it can be useful if you're not getting the response you're expecting. The requests `Response` object has a lot of attributes, but the status code is one of the most useful. \n",
    "\n",
    "**HTTP Status Codes**\n",
    "\n",
    "A full list of status codes can be found [here](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html). The codes are broken up into five categories. They can be summed up as:\n",
    "\n",
    "* 1xx: Informational. The request is not yet complete.\n",
    "* 2xx: Success! The data has posted or the page is retrieved.\n",
    "* 3xx: Redirection. The server is either sending you somewhere else or you need to find a different way to request the page.\n",
    "* 4xx: Client Error. There is an error on your end.\n",
    "* 5xx: Server Error. There is an error on the server's end.\n",
    "\n",
    "Some commonly encountered codes are:\n",
    "\n",
    "* 200 **OK**: The request has succeeded\n",
    "* 301 **Moved Permanently**: The page you are looking for is somewhere else and we'll take you to it. \n",
    "* 302 **Found**: The content you're looking for is actually elsewhere. This is common after logging in.\n",
    "* 400 **Bad Request**: Something is wrong with your request.\n",
    "* 401 **Unauthorized**: You are not authenticated to reach the content.\n",
    "* 403 **Forbidden**: You do not have permission to reach the content.\n",
    "* 404 **Not Found**: The content doesn't exist.\n",
    "* 500 **Internal Server Error**: The server isn't configured correctly or there was another error.\n",
    "* 502 **Bad Gateway**: The gateway or proxy on the server had an error.\n",
    "\n",
    "**Other Parameters**\n",
    "\n",
    "Here are some other parameters that might be useful:\n",
    "\n",
    "* `response.url`: See the source of the content. It might be different from what you supplied depending on the type of request.\n",
    "    * `response.request.url`: See the URL you requested.\n",
    "* `response.headers`: Learn information about the server.\n",
    "    * `response.request.headers` will show you the headers you sent.\n",
    "* `response.elapsed`: See how long the request took to process.\n",
    "* `response.encoding`: Get the character encoding for the content returned. Particularly useful with international sites.\n",
    "* `response.ok`: Tells you if the request was good or if there was an error with the request. (200 `OK`, 204 `NO CONTENT`, 304 `NOT MODIFIED`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status_code: 200\n",
      "\n",
      "url: http://quotes.toscrape.com/\n",
      "\n",
      "headers: {'Server': 'nginx/1.14.0 (Ubuntu)', 'Date': 'Wed, 12 Feb 2020 03:19:16 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Upstream': 'spidyquotes-master_web', 'Content-Encoding': 'gzip'}\n",
      "\n",
      "elapsed: 0:00:00.241531\n",
      "\n",
      "encoding: utf-8\n",
      "\n",
      "ok: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attrs = ['status_code', 'url', 'headers', 'elapsed', 'encoding', 'ok']\n",
    "for attr in attrs:\n",
    "    print(f\"{attr}: {getattr(response, attr)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Data\n",
    "\n",
    "Sometimes you need to send data along with your request. This may be setting search **param**eters in a query or sending form data to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "http://quotes.toscrape.com/?group=PyRVA&speakers=Brian+Cohan&speakers=Sam+Portillo\n"
     ]
    }
   ],
   "source": [
    "params = {'group': 'PyRVA', 'speakers': ['Brian Cohan', 'Sam Portillo']}\n",
    "response = requests.get('http://quotes.toscrape.com/', params=params)\n",
    "print(response.status_code)\n",
    "print(response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Requests to login on a website. You'll have to send some **data** along with the request. All you need to do is supply a dictionary of the required fields.\n",
    "\n",
    "In this example, we get a page on the site we're visiting. The `assert` line verifies that there is a \"Login\" button to click implying we are not logged in. Then a post request is sent with the supplied data to the login url. We can then test that \"Login\" is no longer visible, but \"Logout\" is. This indicates we have successfully logged into the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Response [302]>]\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/')\n",
    "assert 'Login' in response.text and 'Logout' not in response.text\n",
    "\n",
    "data = {'username': 'PyRVA', 'password': 'Rocks'}\n",
    "response = requests.post('http://quotes.toscrape.com/login', data=data)\n",
    "assert 'Login' not in response.text and 'Logout' in response.text\n",
    "print(response.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Sessions\n",
    "\n",
    "Logging into a site is handy, but HTTP is a [stateless protocol](https://en.wikipedia.org/wiki/Stateless_protocol). This means each request knows nothing of any requests before or after it. As you can see, if we login and then try to get another page, we're not logged in anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Logged In Anymore\n"
     ]
    }
   ],
   "source": [
    "data = {'username': 'PyRVA', 'password': 'Rocks'}\n",
    "response = requests.post('http://quotes.toscrape.com/login', data=data)\n",
    "assert 'Login' not in response.text and 'Logout' in response.text\n",
    "\n",
    "response = requests.get('http://quotes.toscrape.com/')\n",
    "if 'Logout' in response.text:\n",
    "    print(\"Still Logged In\")\n",
    "else:\n",
    "    print(\"Not Logged In Anymore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this is the `requests.Session()` class. Create a `session` variable and replace `requests.get()` with `session.get()`. When you are done with the variable, make sure to call `session.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still Logged In\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "data = {'username': 'PyRVA', 'password': 'Rocks'}\n",
    "response = session.post('http://quotes.toscrape.com/login', data=data)\n",
    "assert 'Login' not in response.text and 'Logout' in response.text\n",
    "\n",
    "response = session.get('http://quotes.toscrape.com/')\n",
    "if 'Logout' in response.text:\n",
    "    print(\"Still Logged In\")\n",
    "else:\n",
    "    print(\"Not Logged In Anymore\")\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests.Session()` class also supports [context managers](https://docs.python.org/3/library/contextlib.html) using the [with statement](https://docs.python.org/2.5/whatsnew/pep-343.html). This way, you won't have to worry about closing the session manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still Logged In\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as session:\n",
    "    data = {'username': 'PyRVA', 'password': 'Rocks'}\n",
    "    response = session.post('http://quotes.toscrape.com/login', data=data)\n",
    "    assert 'Login' not in response.text and 'Logout' in response.text\n",
    "\n",
    "    response = session.get('http://quotes.toscrape.com/')\n",
    "    if 'Logout' in response.text:\n",
    "        print(\"Still Logged In\")\n",
    "    else:\n",
    "        print(\"Not Logged In Anymore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Header Data\n",
    "\n",
    "Sometimes you will need to modify the header data of your request. One good example is if a website is [blocking your user-agent](https://geekflare.com/block-unwanted-requests/). As you can see below, requests has a default of `python-requests/2.22.0`. This is a good indicator that you're a scraping bot and it might be blocked. However, you can change the user agent to be whatever you want. Just make sure you're not voilating any terms of service or `robots.txt`. You can find example user agents [here](https://developers.whatismybrowser.com/useragents/explore/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
      "{'User-Agent': 'pretending to be Chrome', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'event': 'PyRVA'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/')\n",
    "print(response.request.headers)\n",
    "\n",
    "headers = {'User-Agent': 'pretending to be Chrome', 'event': 'PyRVA'}\n",
    "response = requests.get('http://quotes.toscrape.com/', headers=headers)\n",
    "print(response.request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a Bad Request to Timeout\n",
    "\n",
    "If you're processing a particularly slow site and you don't want your bot to hang (wait forever) on the line, you can set a timeout. All you have to do is set the `timeout` parameter with your request. As you can see below, the response takes way longer than the really impatient timeout threshold. You would normally want to set your timeouts to something more reasonable, but this was done for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server took more than 1e-06 seconds to respond.\n"
     ]
    }
   ],
   "source": [
    "timeout = 0.000001  # Really impatient\n",
    "try:\n",
    "    requests.get('http://quotes.toscrape.com', timeout=timeout)\n",
    "except requests.exceptions.Timeout:\n",
    "    print(f\"Server took more than {timeout} seconds to respond.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Behind A Proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you are working from an office, there's a decent chance you're working behind a proxy server. All your web traffic must be routed to the proxy to reach the internet. You may also want to use a proxy to hide your IP from the sites you're scraping. You can configure the proxies py passing in a dictionary of protocols along with the IP address or Domain name and the port. For more information, see the [requests proxy documentation](https://requests.readthedocs.io/en/master/user/advanced/#proxies).\n",
    "\n",
    "    proxies = {\n",
    "      'http': 'http://10.10.1.10:3128',\n",
    "      'https': 'http://10.10.1.10:1080',\n",
    "    }\n",
    "\n",
    "    response = requests.get('http://quotes.toscrape.com/', proxies=proxies)\n",
    "\n",
    "If this is a frequent occurence, you can set [environmental variables](https://en.wikipedia.org/wiki/Environment_variable). Requets will automatically look for `HTTP_PROXY` and `HTTPS_PROXY`.\n",
    "\n",
    "Linux\n",
    "\n",
    "    $ export HTTP_PROXY=\"http://10.10.1.10:3128\"\n",
    "    $ export HTTPS_PROXY=\"http://10.10.1.10:1080\"\n",
    "    \n",
    "Windows\n",
    "\n",
    "    $ SET HTTP_PROXY=http://10.10.1.10:3128\n",
    "    $ SET HTTPS_PROXY=http://10.10.1.10:1080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
