{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Page Content \n",
    "\n",
    "Now that we have determined that we need to scrape the web, how the pages are generated, and what tools we have available, the next step is to get the information from the web pages to our machine. First, we need to import the modules we need. The `requests` module should be all that you need, but the `urllib` modules are included for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import urllib.robotparser\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can I Scrape This Site?\n",
    "\n",
    "The first thing you should do is check to see if you're allowed to scrape a website. First we'll try to see if we can scrape some pages from http://quotes.toscrape.com/. Requests does not have a module for checking `robots.txt`, so we'll have to use `urllib.robotparser`. There was an effort to make a plugin called [`requests-robotstxt`](https://github.com/ambv/requests-robotstxt), but it has been inactive since 2017 and was just a proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfp = urllib.robotparser.RobotFileParser()\n",
    "rfp.set_url('http://quotes.toscrape.com/robots.txt')\n",
    "rfp.read()\n",
    "rfp.can_fetch(\"*\", \"http://quotes.toscrape.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have permission! We could check this manually as well, but 1) we want our bots to be well behaived just like their creators, and 2) it appears that they don't even have a `robots.txt` file! Looking at the code below, we see that we get a `404 Error`. Looking at the [list of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes), we see that this means \"Not Found.\"\n",
    "\n",
    "If this is the case, then you might want to do a bit more searching to see if there's any issue. Otherwise, [this article](https://serverfault.com/questions/154820/what-happens-if-a-website-does-not-have-a-robots-txt-file) seems to indicate that no `robots.txt` file means we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/robots.txt')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at another site that does have a `robots.txt` file. It's understandable that Wikipedia get's a lot of traffic. Let's see what bots are allowed to scrape the main page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IsraBot', 'Orthogaffe', '*']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "rfp = urllib.robotparser.RobotFileParser('https://en.wikipedia.org/robots.txt')\n",
    "rfp.read()\n",
    "\n",
    "bots = [\n",
    "    'MJ12bot', 'Mediapartners-Google*', 'IsraBot', 'Orthogaffe', 'UbiCrawler', 'DOC', \n",
    "    'Zao', 'sitecheck.internetseer.com', 'Zealbot', 'MSIECrawler', 'SiteSnagger',\n",
    "    'WebStripper', 'WebCopier', 'Fetch', 'Offline Explorer', 'Teleport', 'TeleportPro', \n",
    "    'WebZIP', 'linko', 'HTTrack', 'Microsoft.URL.Control', 'Xenu', 'larbin', 'libwww', \n",
    "    'ZyBORG', 'Download Ninja', 'fast', 'wget', 'grub-client', 'k2spider', 'NPBot', \n",
    "    'WebReaper', '*'\n",
    "]\n",
    "allowed = [\n",
    "    bot for bot in bots \n",
    "    if rfp.can_fetch(bot, 'https://en.wikipedia.org/')\n",
    "]\n",
    "print(allowed)\n",
    "print(rfp.can_fetch('*', 'https://en.wikipedia.org/w/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only `IsraBot`, `Orthogaffe`, and `*` are allowed to scrape. Fortunately for us, our bot is most likely going to follow the rules for `*`. The other two bots are bots built by Wikipedia. Also worth nothing, we're allowed to scrape `/`, but not `/w/`. If we try to look at that page, we'll see that `/w/` just redirects us to `/` anyway, so we're not missing much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a page\n",
    "\n",
    "Getting a page with `requests` is pretty straight forward. In fact, you already saw a quick example when we saw the `404 Error` earlier. The simplest way to **get** a web page is to use the `requests.get()` function. In the simplest form, all you have to do is pass the function the URL you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There from here, there are several different methods available to extract the response data. The best method depends on data structure.\n",
    "\n",
    "* [`response.text`](https://requests.readthedocs.io/en/master/user/quickstart/#response-content): Decoded text\n",
    "* [`response.content`](https://requests.readthedocs.io/en/master/user/quickstart/#binary-response-content): Binary response content\n",
    "* [`response.json()`](https://requests.readthedocs.io/en/master/user/quickstart/#json-response-content): JSON respone content. Saves you from having to run `json.loads()`\n",
    "* [`response.raw.read()`](https://requests.readthedocs.io/en/master/user/quickstart/#raw-response-content): Raw response content\n",
    "\n",
    "Below you can see the difference between `.text` and `.content`. BeautifulSoup will be able to accept either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Quotes to Scrape</title>\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "print(response.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<title>Quotes to Scrape</title>\\n   '\n"
     ]
    }
   ],
   "source": [
    "print(response.content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Header Data\n",
    "\n",
    "**HTTP Status Codes**\n",
    "\n",
    "A full list of status codes can be found [here](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html). The codes are broken up into five categories. They can be summed up as:\n",
    "\n",
    "* 1xx: Informational. The request is not yet complete.\n",
    "* 2xx: Success! The data has posted or the page is retrieved.\n",
    "* 3xx: Redirection. The server is either sending you somewhere else or you need to find a different way to request the page.\n",
    "* 4xx: Client Error. There is an error on your end.\n",
    "* 5xx: Server Error. There is an error on the server's end.\n",
    "\n",
    "Some commonly encountered codes are:\n",
    "\n",
    "* 200 `OK`: The request has succeeded\n",
    "* 301 `Moved Permanently`: The page you are looking for is somewhere else and we'll take you to it. \n",
    "* 302 `Found`: The content you're looking for is actually elsewhere. This is common after logging in.\n",
    "* 400 `Bad Request`: Something is wrong with your request.\n",
    "* 401 `Unauthorized`: You are not authenticated to reach the content.\n",
    "* 403 `Forbidden`: You do not have permission to reach the content.\n",
    "* 404 `Not Found`: The content doesn't exist.\n",
    "* 500 `Internal Server Error`: The server isn't configured correctly or there was another error.\n",
    "* 502 `Bad Gateway`: The gateway or proxy on the server \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://quotes.toscrape.com/\n",
      "\n",
      "status_code: 200\n",
      "\n",
      "headers: {'Server': 'nginx/1.14.0 (Ubuntu)', 'Date': 'Mon, 10 Feb 2020 01:49:55 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Upstream': 'spidyquotes-master_web', 'Content-Encoding': 'gzip'}\n",
      "\n",
      "elapsed: 0:00:00.241843\n",
      "\n",
      "encoding: utf-8\n",
      "\n",
      "ok: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attrs = ['url', 'status_code', 'headers', 'elapsed', 'encoding', 'ok']\n",
    "for attr in attrs:\n",
    "    print(f\"{attr}: {getattr(response, attr)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Data\n",
    "\n",
    "Sometimes you need to send data along with your request. This may be setting search parameters in a query or sending form data to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "http://quotes.toscrape.com/?group=PyRVA&speakers=Brian+Cohan&speakers=Sam+Portillo\n"
     ]
    }
   ],
   "source": [
    "params = {'group': 'PyRVA', 'speakers': ['Brian Cohan', 'Sam Portillo']}\n",
    "response = requests.get('http://quotes.toscrape.com/', params=params)\n",
    "print(response.status_code)\n",
    "print(response.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Response [302]>]\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/', params=params)\n",
    "assert 'Login' in response.text and 'Logout' not in response.text\n",
    "\n",
    "data = {'username': 'PyRVA', 'password': 'Rocks'}\n",
    "response = requests.post('http://quotes.toscrape.com/login', data=data)\n",
    "assert 'Login' not in response.text and 'Logout' in response.text\n",
    "print(response.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Logged In Anymore\n"
     ]
    }
   ],
   "source": [
    "data = {'username': 'PyRVA', 'password': 'Rocks'}\n",
    "response = requests.post('http://quotes.toscrape.com/login', data=data)\n",
    "assert 'Login' not in response.text and 'Logout' in response.text\n",
    "\n",
    "response = requests.get('http://quotes.toscrape.com/')\n",
    "if 'Logout' in response.text:\n",
    "    print(\"Still Logged In\")\n",
    "else:\n",
    "    print(\"Not Logged In Anymore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still Logged In\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "data = {'username': 'PyRVA', 'password': 'Rocks'}\n",
    "response = session.post('http://quotes.toscrape.com/login', data=data)\n",
    "assert 'Login' not in response.text and 'Logout' in response.text\n",
    "\n",
    "response = session.get('http://quotes.toscrape.com/')\n",
    "if 'Logout' in response.text:\n",
    "    print(\"Still Logged In\")\n",
    "else:\n",
    "    print(\"Not Logged In Anymore\")\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still Logged In\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as session:\n",
    "    data = {'username': 'PyRVA', 'password': 'Rocks'}\n",
    "    response = session.post('http://quotes.toscrape.com/login', data=data)\n",
    "    assert 'Login' not in response.text and 'Logout' in response.text\n",
    "\n",
    "    response = session.get('http://quotes.toscrape.com/')\n",
    "    if 'Logout' in response.text:\n",
    "        print(\"Still Logged In\")\n",
    "    else:\n",
    "        print(\"Not Logged In Anymore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Header Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
      "{'User-Agent': 'pretending to be Chrome', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'event': 'PyRVA'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://quotes.toscrape.com/')\n",
    "print(response.request.headers)\n",
    "\n",
    "headers = {'User-Agent': 'pretending to be Chrome', 'event': 'PyRVA'}\n",
    "response = requests.get('http://quotes.toscrape.com/', headers=headers)\n",
    "print(response.request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a Bad Request to Timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server took more than 2 seconds to respond.\n"
     ]
    }
   ],
   "source": [
    "timeout = 2\n",
    "try:\n",
    "    requests.get('http://quotes.toscrape.com:81', timeout=timeout)\n",
    "except requests.exceptions.Timeout:\n",
    "    print(f\"Server took more than {timeout} seconds to respond.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Behind A Proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    proxies = {\n",
    "      'http': 'http://10.10.1.10:3128',\n",
    "      'https': 'http://10.10.1.10:1080',\n",
    "    }\n",
    "\n",
    "    response = requests.get('http://quotes.toscrape.com/', proxies=proxies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
