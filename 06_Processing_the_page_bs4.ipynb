{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Webpages with BeautifulSoup\n",
    "\n",
    "Welcome! This module will be a walkthrough to processing web data with the popular Python package BeautifulSoup.\n",
    "\n",
    "BeautifulSoup has a terrific [webpage for documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) that has in-depth installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood of a webpage\n",
    "\n",
    "![](images/python_reddit.png)\n",
    "\n",
    "Every part of a webpage is generated from the underlying HTML. BeautifulSoup makes it easy to get this data and do cool things with it.\n",
    "\n",
    "#### Fortunately\n",
    "You don't need to know HTML to use BeautifulSoup, but it certainly helps.\n",
    "For example, if you know what you want from looking at the webpage, you may not understand how to HTML works underneath which will limit what your efficiency with BeautifulSoup. Otherwise, the more about HTML you know, the more effective a tool BeautifulSoup will be.\n",
    "<br><br><br>\n",
    "### Creating a soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the soup object \n",
    "url = 'https://www.reddit.com/r/python'\n",
    "r  = requests.get(url)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing the soup will print the full soup object [very long][not pretty]\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second try [also long][significantly prettier]\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup object can be iterated\n",
    "for tag in soup.find_all():\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a specific tag\n",
    "h1_tag = soup.find('h1')\n",
    "print(h1_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the object types\n",
    "print('soup type:',type(soup),'\\n')\n",
    "print('tag type:',type(h1_tag),'\\n')\n",
    "print('tag attributes:',h1_tag.attrs,'\\n')\n",
    "print('tag text type',type(h1_tag.string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all of the a tags (links)\n",
    "a_tag_list = []\n",
    "for tag in soup.find_all('a'):\n",
    "    a_tag_list.append(tag)\n",
    "    \n",
    "a_tag_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in a_tag_list:\n",
    "    print(tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it looks like theres a predictable structure in reddit's webpage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lets iterate and look for div tags that contain the word 'project'\n",
    "tag_list = []\n",
    "for tag in soup.find_all('div'):\n",
    "    if 'python' in tag.get_text().lower():\n",
    "        tag_list.append(tag)\n",
    "\n",
    "example_tag = tag_list[0]\n",
    "\n",
    "example_tag # pretty ugly\n",
    "example_tag.prettify() # still terrible\n",
    "example_tag.get_text() # still terrible somehow\n",
    "\n",
    "for tag in example_tag.find_all():\n",
    "    if 'python' in tag.get_text().lower() and len(str(tag)) < 200:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move sideways\n",
    "example_tag.next_sibling\n",
    "example_tag.previous_sibling\n",
    "\n",
    "# move up\n",
    "print(type(example_tag.parent))\n",
    "print(type(example_tag.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of jk rowling quotes\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "r  = requests.get(url)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "for tag in soup.find_all('small'):\n",
    "    if tag.get_text() == 'J.K. Rowling':\n",
    "        try:\n",
    "            tag.parent.parent.decompose()\n",
    "            print('successfully decomposed tag')\n",
    "        except Exception as e:\n",
    "            print(e,'could not decompose tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull tags based on attributes\n",
    "for tag in soup.find_all():\n",
    "    if 'class' in tag.attrs and 'author' in tag.attrs['class']:\n",
    "        print(tag.get_text())\n",
    "        \n",
    "    # don't do this because the attribute value may be a list\n",
    "    #if 'class' in tag.attrs and tag.attrs['class'] == 'author':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the same tag every scrape by specifying a selector\n",
    "tag = soup.select('body > div > div:nth-child(2) > div.col-md-4.tags-box > span:nth-child(2) > a')\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error handling\n",
    "\n",
    "# check to ensure the site isnt down\n",
    "print(str(r))\n",
    "\n",
    "# when in doubt, check the data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/http_responses.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important distinction\n",
    "\n",
    "`soup.find_all()` returns all tags<br>\n",
    "`soup.find()` returns one tag<br>\n",
    "`soup.select_one()` returns the first tag matching the selector<br>\n",
    "`soup.select()` returns all tags matching the selector<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
